{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo un chatbot con NLTK desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el corpus con el texto para el chatbot\n",
    "\n",
    "Podemos utilizar una página de Wikipedia para crear el corpus. Podemos copiar el contenido en un fichero de texto.\n",
    "\n",
    "Debemos procesar el texto para para convertirlo en datos que el algoritmo pueda interpretar. Por ejemplo:\n",
    "\n",
    "* Convertir el texto a mayúsculas/minúsculas para que el algoritmo no diferencie los token.\n",
    "* Tokenización: NLTK permite tokenizar por frases (sentence tokenizer) y por palabras (word tokenizer)\n",
    "\n",
    "El paquete NLTK incluye:\n",
    "\n",
    "* Listas de stopwords\n",
    "* Stemming: obtener la raíz de una palabra. Por ejemplo, el algoritmo de Porter es un algoritmo de derivación de palabras y consiste en elimar las términaciones morfológicas comunes de las palabras en inglés, aunque es posible usarlo también en español. Es un proceso de normalización de términos muy usado en corpus basados en texto. Por ejemplo, texto lo transforma a text y canta a cant.\n",
    "* Lematización: relaciona una palabra flexionada o derivada con su forma canónica o lema. Por ejemplo, cantas, canto lo convierte a cantar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('chatbot-es.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando\n",
    "\n",
    "Analizamos los tokens para normalizarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    #return [lemmer.lemmatize(token) for token in tokens]\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para saludar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hola\", \"que tal?\")\n",
    "GREETING_RESPONSES = [\"Buenos días\", \"hola\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generamos la respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Después del preproceso inicial, transformamos el texto en un vector (array) de números. El bag-of-words es la representación del texto que describe las ocurrencias de las palabras en un documento incluyendo el vocabulario de palabras y el número de ocurrencias. No se tiene en cuenta ni dónde ocurren ni el orden, solo tiene en cuenta si las palabras aparecen en el texto.\n",
    "\n",
    "En el caso de bag-of-words los documentos se consideran similares si contienen texto similar. Por ejemplo, {Gustavo, juega, al, fútbol} y vectorizamos el texto \"Gustavo juega\", tendríamos el siguiente vector: (1,1,0,0).\n",
    "\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "Term Frequency: la frecuenca de las palabras en un documento.\n",
    "\n",
    "TF = (Número de veces que el término t aparece en un documento)/(Número de términos del documento)\n",
    "\n",
    "Inverse Document Frequency: mide la relevancia de una palabra para un documento en una colección.\n",
    "\n",
    "IDF = 1+log(N/n), donde, N el número de documentos y n es el número de documentos donde el térmimo t aparece.\n",
    "\n",
    "\n",
    "**Similitud del coseno**\n",
    "Es una medida de la similitud existente entre dos vectores. Mide el ángulo entre dos vectores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar una respuesta los conceptos de similitud se tienen en cuenta. Definimos una función de respuesta que busca una o más palabras y devuelve una posible respuesta. Si no encuentra nada devuelve: ”Lo siento! No entiendo tu pregunta”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword list to use\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords.words(\"spanish\"))\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"Lo siento! No entiendo tu pregunta\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, añadimos una función para solicitar el texto por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "hola\n",
      "ROBO: Buenos días\n",
      "que es un chatbot?\n",
      "ROBO: un bot de charla o bot conversacional (en inglés, chatbot) es un programa que simula mantener una conversación con una persona al proveer respuestas automáticas a entradas hechas por el usuario.\n",
      "para qué sirve un chatbot?\n",
      "ROBO: para crear un chatbot existen páginas que ofrecen plantillas con tecnología de procesamiento de lenguaje natural (pln), la parte más compleja a la hora de desarrollarlo, ya que requiere conocimiento de programación.5​\n",
      "\n",
      "actualmente, la producción de estos bots se ha elevado notablemente.\n",
      "quien es siri?\n",
      "ROBO: Lo siento! No entiendo tu pregunta\n",
      "siri?\n",
      "ROBO: Lo siento! No entiendo tu pregunta\n",
      "Joseph Weizenbaum?\n",
      "ROBO: los chatbots tienen su origen en 1966, cuando joseph weizenbaum creó el primer chatbot basado en inteligencia artificial, eliza, que actuaba como terapeuta.\n",
      "que es eliza?\n",
      "ROBO: los bots más clásicos y tempranos, además de eliza, son smarterchild, parry y shrdlu; entre los más recientes se encuentran racter, a.l.i.c.e o jabberwacky, dr. abuse (de barres&boronat), muy parecido a eliza y semantycs (de full on net) que ya puede interactuar con diversos canales y extraer información de erp.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='Gracias' or user_response=='Muchas gracias' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: De nada.\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Hasta luego!..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Referencias\n",
    "\n",
    "* https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK/blob/master/Chatbot.ipynb\n",
    "* https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e\n",
    "* [Intro NLP](http://josearcosaneas.github.io/python/r/procesamiento/lenguaje/2017/01/02/procesamiento-lenguaje-natural-0.html#:~:text=El%20stemming%20consiste%20en%20extreaer,ayuda%20de%20la%20librer%C3%ADa%20NLTK.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
